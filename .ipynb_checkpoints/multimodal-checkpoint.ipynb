{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7770503e-1773-499b-ad05-6bf9cc94c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def setup_gpu_memory_growth():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            # 打印异常信息\n",
    "            print(e)\n",
    "\n",
    "# 在程序的开始调用这个函数\n",
    "setup_gpu_memory_growth()\n",
    "\n",
    "# 以下是你的其他TensorFlow代码\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c97de78-cfda-4bc7-9da4-30fe33b9bbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# 现在可以运行你的TensorFlow代码了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69879ea-c334-48be-81bb-71cc1c288762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cbf550a-ac6a-4af0-8df5-bae8c61ab70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e186a1b2-a1f9-460b-9782-86480dc98440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4362a9-3b08-4cd5-b598-202d1069086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276a8d4c-cfb1-4522-98fe-e3be71493cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.optimizers as optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf52d0d-76f5-4169-a99f-15b203edb283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.utils as kutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f67543-fb74-4c10-9322-5caf602a4384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448afae-8eca-4080-bd78-b0372099f39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0d6d5-0552-4d2b-99e2-d66567e85aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780987f-98ad-4ebd-9df2-e290724d999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688abaa-fcf4-4127-bfd5-718ac927d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3725b710-b714-4e25-999c-512b0da7fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55d69b-7fb1-4504-ae64-1e83ee6ae2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf049ec9-5f02-4b68-a03c-c9acf90560ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b0bcc2-8d20-4790-8558-5e7e50330c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e9b236-84a5-43ba-91be-bbf5c500680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import losses\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2490f-869e-4508-a641-472fd9434a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fusion import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da5afb-9e0c-43f6-a076-618e6fc652ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fusion  ######改成py！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b1426-509b-4458-95f9-56d8395aca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####optical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcb841a-3b21-4616-b826-ed6c9334a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 定义数据集目录\n",
    "BASE_DIR = \"/home/jovyan/private/thesis_data/code/final_code/dataset\"\n",
    "# 假设您有三个不同的目录，分别存储不同类型的图像\n",
    "S1_IMAGE_DIR = os.path.join(BASE_DIR, \"sentinel1_images\")\n",
    "S2_10_IMAGE_DIR = os.path.join(BASE_DIR, \"sentinel2_10_images\")\n",
    "S2_20_IMAGE_DIR = os.path.join(BASE_DIR, \"sentinel2_20_images\")\n",
    "GEDI_IMAGE_DIR = os.path.join(BASE_DIR, \"gedi_4\")\n",
    "LABEL_250_DIR = os.path.join(BASE_DIR, \"LABEL_250_DIR\")\n",
    "LABEL_100_DIR = os.path.join(BASE_DIR, \"LABEL_100_DIR\")\n",
    "\n",
    "\n",
    "# 定义常量\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1000\n",
    "STEPS = 500\n",
    "\n",
    "# 定义测试和验证图像的文件名\n",
    "# 假设每种类型的图像都有相同的命名方式\n",
    "TRAIN_IMAGES = [\"2.tif\", \"7.tif\", \"4.tif\", \"5.tif\", \"9.tif\"]\n",
    "VAL_IMAGES = [\"6.tif\", \"3.tif\"]\n",
    "TEST_IMAGES = [\"8.tif\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31e4d1-3eab-4d46-8fc9-e6f3cf00a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_sizes = {\"s1\": (224, 224), \"s2_10\": (224, 224),\"s2_20\":(112,112), \"gedi\": (28, 28), \"LABEL_250_DIR\": (9, 9), \"LABEL_100_DIR\": (23, 23)}\n",
    "bands_count = {\"s1\": 6, \"s2_10\": 4,\"s2_20\":6,\"gedi\":4 }\n",
    "label_dirs = ['LABEL_250_DIR', 'LABEL_100_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0361438-023f-49ab-90ba-62c9996176cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####read multiband image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398537c-4a34-45aa-9499-f94d047e7096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess_image(image_path, normalise_percentiles=(0.5, 99), is_complex=False):\n",
    "    with rasterio.open(image_path) as image_file:\n",
    "        image_arr = image_file.read()  # 以[通道, 高度, 宽度]格式读取所有波段\n",
    "       \n",
    "        image_arr[image_arr == image_file.nodata] = np.nan\n",
    "                # 如果提供了模板，则应用模板\n",
    "        # if template is not None:\n",
    "        #     image_arr = apply_template(image_arr, template)\n",
    "        \n",
    "        if normalise_percentiles:\n",
    "            for band in range(image_arr.shape[0]):\n",
    "                band_arr = image_arr[band, :, :]\n",
    "                min_value = np.nanpercentile(band_arr, normalise_percentiles[0])\n",
    "                max_value = np.nanpercentile(band_arr, normalise_percentiles[1])\n",
    "                if max_value - min_value > np.finfo(float).eps:\n",
    "                    image_arr[band, :, :] = (band_arr - min_value) / (max_value - min_value)\n",
    "                else:\n",
    "                    image_arr[band, :, :] = np.zeros_like(band_arr)\n",
    "                    \n",
    "                    \n",
    "        if is_complex:\n",
    "            # 对于复数数据的特殊处理\n",
    "            real_channel_1 = np.expand_dims(image_arr[0], axis=0)  # 第一个通道，扩展维度以便堆叠\n",
    "            imag_channel_1 = np.zeros_like(real_channel_1)  # 第一个通道的虚部，使用零填充\n",
    "            \n",
    "            complex_real = np.expand_dims(image_arr[1], axis=0)  # 第二个通道作为复数的实部\n",
    "            complex_imag = np.expand_dims(image_arr[2], axis=0)  # 第三个通道作为复数的虚部\n",
    "            \n",
    "            real_channel_4 = np.expand_dims(image_arr[3], axis=0)  # 第四个通道\n",
    "            imag_channel_4 = np.zeros_like(real_channel_4)  # 第四个通道的虚部，使用零填充\n",
    "            \n",
    "            # 沿通道轴堆叠以创建6个通道\n",
    "            image_arr = np.concatenate([real_channel_1, imag_channel_1, complex_real, complex_imag, real_channel_4, imag_channel_4], axis=0)\n",
    "\n",
    "        # 在返回之前将 NaN 和 Inf 转换为 0\n",
    "        # 此处 posinf 和 neginf 参数可用于指定用于替换正、负无穷的值；默认行为是用最大或最小的浮点数替换\n",
    "        image_arr = np.nan_to_num(image_arr, nan=0.0, posinf=None, neginf=None)\n",
    "\n",
    "    return image_arr.transpose(1, 2, 0)  # 重新排序维度为高度 x 宽度 x 通道\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41120f2c-d4e5-4211-bbe8-e1d215f8880f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_single_band_image(image_path, normalise_percentiles=(0, 100)):\n",
    "    with rasterio.open(image_path) as image_file:\n",
    "        image_arr = image_file.read(1)  # Reads the first band\n",
    "        \n",
    "        # Checks and replaces nodata values with np.nan\n",
    "        if image_file.nodata is not None:\n",
    "            image_arr[image_arr == image_file.nodata] = np.nan\n",
    "        \n",
    "        # Applies normalization if needed\n",
    "        if normalise_percentiles:\n",
    "            min_value = np.nanpercentile(image_arr, normalise_percentiles[0])\n",
    "            max_value = np.nanpercentile(image_arr, normalise_percentiles[1])\n",
    "            if max_value - min_value > np.finfo(float).eps:\n",
    "                image_arr = (image_arr - min_value) / (max_value - min_value)\n",
    "            else:\n",
    "                image_arr = np.zeros_like(image_arr)\n",
    "        \n",
    "        # Converts np.nan to 0.0\n",
    "        single_band_image = np.nan_to_num(image_arr, nan=0.0)\n",
    "    \n",
    "    # Adds a channel dimension and returns\n",
    "    return np.expand_dims(single_band_image, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58aa231-15f6-4e9d-aafe-68338835f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#反归一化：在某些情况下，特别是回归问题中，你可能需要将预测结果“反归一化”回原始的数值范围，以便结果更易于理解和使用。这要求保存归一化过程中使用的参数（如最小值和最大值），以便在预测后进行逆操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1d849-272a-4131-96ab-7011035702cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####别忘了patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504e62d-80ff-4e72-b0a4-3152510cb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image_to_patch_size(image, patch_size):\n",
    "    height, width, depth = image.shape\n",
    "    patch_height, patch_width = patch_size  # 解包元组，分别获取补丁的高度和宽度\n",
    "\n",
    "    # 计算目标高度和宽度\n",
    "    target_height = patch_height * (height // patch_height + (1 if height % patch_height != 0 else 0))\n",
    "    target_width = patch_width * (width // patch_width + (1 if width % patch_width != 0 else 0))\n",
    "\n",
    "    # 计算填充量\n",
    "    pad_height = (target_height - height) // 2\n",
    "    pad_width = (target_width - width) // 2\n",
    "\n",
    "    # 创建填充后的图像\n",
    "    padded = np.zeros((target_height, target_width, depth), dtype=image.dtype)\n",
    "    padded[pad_height:pad_height + height, pad_width:pad_width + width, :] = image\n",
    "\n",
    "    return padded, (pad_height, pad_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e9e41-3a55-43a2-adf5-28748da61a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSequence(kutils.Sequence):\n",
    "    def __init__(self, filenames, batch_size, patch_sizes, bands_count, len, label_dirs):\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_sizes = patch_sizes  # 字典，包含每种图像和标签的补丁大小\n",
    "        self.bands_count = bands_count  # 字典，包含每种图像类型的波段数\n",
    "        self.len = len\n",
    "        self.label_dirs = ['LABEL_250_DIR', 'LABEL_100_DIR']  # 列表，包含标签的目录\n",
    "        self.read_dataset()\n",
    "\n",
    "    def read_dataset(self):\n",
    "        self.data = []\n",
    "        for filename in self.filenames:\n",
    "            # 读取和处理图像\n",
    "            s1_image_path = os.path.join(S1_IMAGE_DIR, filename)\n",
    "            s2_10_image_path = os.path.join(S2_10_IMAGE_DIR, filename)\n",
    "            s2_20_image_path = os.path.join(S2_20_IMAGE_DIR, filename)\n",
    "            gedi_image_path = os.path.join(GEDI_IMAGE_DIR, filename)\n",
    "            \n",
    "            #s1_image = read_and_preprocess_image(s1_image_path, normalise=True)\n",
    "            s1_image = read_and_preprocess_image(s1_image_path, is_complex=True)\n",
    "\n",
    "            \n",
    "            s2_10_image = read_and_preprocess_image(s2_10_image_path)\n",
    "            s2_20_image = read_and_preprocess_image(s2_20_image_path)\n",
    "            gedi_image = read_and_preprocess_image(gedi_image_path)\n",
    "            \n",
    "            # Pad each image according to its patch size\n",
    "            s1_image, _ = pad_image_to_patch_size(s1_image, self.patch_sizes['s1'])\n",
    "            s2_10_image, _ = pad_image_to_patch_size(s2_10_image, self.patch_sizes['s2_10'])\n",
    "            s2_20_image, _ = pad_image_to_patch_size(s2_20_image, self.patch_sizes['s2_20'])\n",
    "            gedi_image, _ = pad_image_to_patch_size(gedi_image, self.patch_sizes['gedi'])\n",
    "\n",
    "\n",
    "            images = {'s1': s1_image, 's2_10': s2_10_image, 's2_20':s2_20_image, 'gedi': gedi_image}\n",
    "\n",
    "            # 读取和处理标签\n",
    "            labels = []\n",
    "            for label_type in self.label_dirs:\n",
    "                label_path = os.path.join(BASE_DIR, label_type, filename)\n",
    "                label = read_single_band_image(label_path)\n",
    "                # 使用 label_type 作为键来获取对应的补丁大小\n",
    "                label, _ = pad_image_to_patch_size(label, self.patch_sizes[label_type])\n",
    "                labels.append(label)\n",
    "\n",
    "\n",
    "            self.data.append((images, labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = {key: np.empty((self.batch_size, *self.patch_sizes[key], self.bands_count[key])) for key in ['s1', 's2_10', 's2_20', 'gedi']}\n",
    "        batch_y = [np.empty((self.batch_size, *self.patch_sizes[label_dir], 1)) for label_dir in self.label_dirs]\n",
    "\n",
    "        for patch_index in range(self.batch_size):\n",
    "            images, labels = self.sample_image()\n",
    "\n",
    "            # 基于参考图像（例如s1）计算采样起始点的比例\n",
    "            ref_image = images['s1']\n",
    "            y_ratio, x_ratio = self.calculate_sampling_start_point(ref_image, self.patch_sizes['s1'])\n",
    "\n",
    "            for key, image in images.items():\n",
    "                patch_size = self.patch_sizes[key]\n",
    "                batch_x[key][patch_index] = self.sample_patch(image, patch_size, y_ratio, x_ratio)\n",
    "\n",
    "            for i, label_dir in enumerate(self.label_dirs):\n",
    "                label = labels[i]\n",
    "                label_patch_size = self.patch_sizes[label_dir]\n",
    "                batch_y[i][patch_index] = self.sample_patch(label, label_patch_size, y_ratio, x_ratio)\n",
    "\n",
    "        return [batch_x[key] for key in batch_x], batch_y\n",
    "\n",
    "    def sample_image(self):\n",
    "        # 随机选择一个图像和标签集\n",
    "        image_index = np.random.choice(len(self.data))\n",
    "        selected_data = self.data[image_index]\n",
    "        images = selected_data[0]  # 第一个元素是图像的字典\n",
    "        labels = selected_data[1]  # 第二个元素是标签的列表\n",
    "        # 打印选中的image_index\n",
    "        #print(\"Selected image index:\", image_index)\n",
    "        return images, labels  # 返回图像字典和标签列表\n",
    "\n",
    "    def sample_patch(self, image, patch_size, y_ratio, x_ratio):\n",
    "        # 根据提供的比例和补丁大小采样补丁\n",
    "        height, width, _ = image.shape\n",
    "        y_start = int((height - patch_size[0]) * y_ratio)\n",
    "        x_start = int((width - patch_size[1]) * x_ratio)\n",
    "        return image[y_start:y_start + patch_size[0], x_start:x_start + patch_size[1], :]\n",
    "\n",
    "    def calculate_sampling_start_point(self, reference_image, reference_patch_size):\n",
    "        # 基于参考图像和参考补丁大小计算采样起始点的比例\n",
    "        height, width, _ = reference_image.shape\n",
    "        y_ratio = np.random.uniform(0, 1 - reference_patch_size[0] / height)\n",
    "        x_ratio = np.random.uniform(0, 1 - reference_patch_size[1] / width)\n",
    "        return y_ratio, x_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598367b1-13cf-4e20-88b8-9c83b9381d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = TrainSequence(TRAIN_IMAGES, BATCH_SIZE, patch_sizes, bands_count, STEPS, label_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578c81e-e699-45b4-b6df-be480f694e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_nan_or_inf_in_batch(data):\n",
    "    #\"\"\"检查批次数据中是否包含NaN或Inf值。\"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        # 如果数据是字典类型，则遍历每个键\n",
    "        for key, value in data.items():\n",
    "            if np.any(np.isnan(value)) or np.any(np.isinf(value)):\n",
    "                return True\n",
    "    elif isinstance(data, list):\n",
    "        # 如果数据是列表\n",
    "        for item in data:\n",
    "            if isinstance(item, np.ndarray):\n",
    "                if np.any(np.isnan(item)) or np.any(np.isinf(item)):\n",
    "                    return True\n",
    "            else:\n",
    "                # 对于嵌套的列表或其它结构，需要进一步检查或适当处理\n",
    "                pass\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        # 对于直接的NumPy数组\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# 假设 train_sequence 是你的 TrainSequence 实例\n",
    "for idx in range(len(train_sequence)):\n",
    "    batch_x, batch_y = train_sequence[idx]  # 获取批次数据\n",
    "    \n",
    "    # 检查特征数据和标签数据中是否有 NaN 或 Inf 值\n",
    "    if check_nan_or_inf_in_batch(batch_x):\n",
    "        print(f\"NaN or Inf detected in features of batch {idx}\")\n",
    "    if check_nan_or_inf_in_batch(batch_y):\n",
    "        print(f\"NaN or Inf detected in labels of batch {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba09e345-3443-410a-95af-ca1b0e43dcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_sequence[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ead641-98bd-47ee-83f1-f13fe63990f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 x 进行检查，这里假设 x 是一个列表或数组的列表\n",
    "for i, item in enumerate(x):\n",
    "    if np.isnan(item).any():\n",
    "        print(f\"NaN value detected in x at index {i}\")\n",
    "\n",
    "# 对 y 进行检查，这里假设 y 是一个列表或数组的列表\n",
    "for i, item in enumerate(y):\n",
    "    if np.isnan(item).any():\n",
    "        print(f\"NaN value detected in y at index {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155dc11-8334-483c-bd5c-f1cceb9796b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印x的类型和内容示例\n",
    "print(type(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82557aab-bf8b-4bdc-aa92-c42c115518b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # x 是一个列表，y 是一个标签列表的列表\n",
    "# # 假设x中的顺序与bands_count中的键顺序一致\n",
    "\n",
    "# # 计算绘图的总行数：所有输入的波段数 + 所有输出的数量\n",
    "# input_band_count = sum(bands_count.values())\n",
    "# output_label_count = sum(len(labels) for labels in y)\n",
    "# nrows = input_band_count + output_label_count\n",
    "# ncols = 1  # 因为我们只查看第一个批次的数据\n",
    "\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(5, nrows * 2), squeeze=False)\n",
    "\n",
    "# current_row = 0\n",
    "# # 可视化每个输入图像的所有波段\n",
    "# for input_index, (input_key, band_count) in enumerate(bands_count.items()):\n",
    "#     for band_index in range(band_count):\n",
    "#         ax = axes[current_row, 0]\n",
    "#         ax.imshow(x[input_index][0, :, :, band_index], cmap='gray')\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title(f'Input: {input_key}, Band: {band_index + 1}')\n",
    "#         current_row += 1\n",
    "\n",
    "# # 可视化每个输出标签\n",
    "# for label_set_index, label_set in enumerate(y):  # y 是标签列表的列表\n",
    "#     for label_index, label_data in enumerate(label_set):\n",
    "#         ax = axes[current_row, 0]\n",
    "#         ax.imshow(label_data[:, :, 0], cmap='viridis')  # 这里做了修正\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title(f'Output Label Set: {label_set_index + 1}, Label: {label_index + 1}')\n",
    "#         current_row += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77248a4b-9b35-4f67-b91e-da743ee5c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import keras.utils as kutils\n",
    "\n",
    "# class ValSequence(kutils.Sequence):\n",
    "#     def __init__(self, filenames, batch_size, patch_sizes, bands_count, label_dirs):\n",
    "#         self.filenames = filenames\n",
    "#         self.batch_size = batch_size\n",
    "#         self.patch_sizes = patch_sizes\n",
    "#         self.bands_count = bands_count\n",
    "#         self.label_dirs = label_dirs\n",
    "#         self.read_dataset()\n",
    "#         self.reset()\n",
    "\n",
    "#     def read_dataset(self):\n",
    "#         self.data = []\n",
    "#         min_patches_count = None\n",
    "\n",
    "#         for filename in self.filenames:\n",
    "#             # 读取和处理图像\n",
    "#             s1_image_path = os.path.join(S1_IMAGE_DIR, filename)\n",
    "#             s2_10_image_path = os.path.join(S2_10_IMAGE_DIR, filename)\n",
    "#             s2_20_image_path = os.path.join(S2_20_IMAGE_DIR, filename)\n",
    "#             gedi_image_path = os.path.join(GEDI_IMAGE_DIR, filename)\n",
    "            \n",
    "#             #s1_image = read_and_preprocess_image(s1_image_path, normalise=True)\n",
    "#             s1_image = read_and_preprocess_image(s1_image_path, is_complex=True)\n",
    "\n",
    "#             # 然后处理其他图像，并使用S1图像作为模板\n",
    "#             s2_10_image = read_and_preprocess_image(s2_10_image_path)\n",
    "#             s2_20_image = read_and_preprocess_image(s2_20_image_path)\n",
    "#             gedi_image = read_and_preprocess_image(gedi_image_path)\n",
    "            \n",
    "#             images = {'s1': s1_image, 's2_10': s2_10_image, 's2_20':s2_20_image, 'gedi': gedi_image}\n",
    "\n",
    "#             labels = {}\n",
    "#             for label_type in self.label_dirs:\n",
    "#                 label_path = os.path.join(BASE_DIR, label_type, filename)\n",
    "#                 label = read_single_band_image(label_path)\n",
    "#                 labels[label_type] = label\n",
    "\n",
    "\n",
    "#             # Pad images and labels to their respective patch sizes\n",
    "#             for key in images:\n",
    "#                 images[key], _ = pad_image_to_patch_size(images[key], self.patch_sizes[key])\n",
    "#             for key in labels:\n",
    "#                 labels[key], _ = pad_image_to_patch_size(labels[key], self.patch_sizes[key])\n",
    "\n",
    "#             # Calculate patches count for each image and label, then update min_patches_count\n",
    "#             all_sizes = {**images, **labels}  # Combine images and labels for unified processing\n",
    "#             for key, item in all_sizes.items():\n",
    "#                 patch_size = self.patch_sizes[key]\n",
    "#                 patches_count = (item.shape[0] // patch_size[0]) * (item.shape[1] // patch_size[1])\n",
    "#                 if min_patches_count is None or patches_count < min_patches_count:\n",
    "#                     min_patches_count = patches_count\n",
    "\n",
    "#             self.data.append((images, labels))\n",
    "\n",
    "#         self.len = min_patches_count * len(self.filenames) // self.batch_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.len\n",
    "    \n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         batch_x = {key: np.empty((self.batch_size, *self.patch_sizes[key], self.bands_count[key])) for key in ['s1', 's2_10', 's2_20', 'gedi']}\n",
    "#         batch_y = {key: np.empty((self.batch_size, *self.patch_sizes[key], 1)) for key in self.label_dirs}\n",
    "\n",
    "#         # 为整个batch计算一个统一的采样起始点\n",
    "#         images, labels = self.data[idx % len(self.data)]\n",
    "#         ref_image = list(images.values())[0]  # 使用第一个图像作为参考\n",
    "#         patch_size = list(self.patch_sizes.values())[0]  # 使用第一个patch size作为参考\n",
    "#         height, width, _ = ref_image.shape\n",
    "#         max_y = height - patch_size[0]\n",
    "#         max_x = width - patch_size[1]\n",
    "#         y = np.random.randint(0, max_y) if max_y > 0 else 0\n",
    "#         x = np.random.randint(0, max_x) if max_x > 0 else 0\n",
    "\n",
    "#         for i in range(self.batch_size):\n",
    "#             for key, image in images.items():\n",
    "#                 batch_x[key][i, ...] = image[y:y + self.patch_sizes[key][0], x:x + self.patch_sizes[key][1], :]\n",
    "#             for key, label in labels.items():\n",
    "#                 batch_y[key][i, ...] = label[y:y + self.patch_sizes[key][0], x:x + self.patch_sizes[key][1], :]\n",
    "\n",
    "#         return [batch_x[key] for key in batch_x], [batch_y[key] for key in batch_y]\n",
    "\n",
    "\n",
    "#     def sample_image(self):\n",
    "#         image_index = np.random.choice(len(self.data))\n",
    "#         return self.data[image_index]\n",
    "\n",
    "#     def reset(self):\n",
    "#         # Resets the sequence to the start\n",
    "#         self.image_index = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fb6a1-f4e9-4a03-82f5-0a3199f49b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sequence =TrainSequence(VAL_IMAGES,BATCH_SIZE, patch_sizes, bands_count, STEPS, label_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9a3a37-874a-4ac7-8663-ba4b459339ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 train_sequence 是你的 TrainSequence 实例\n",
    "for idx in range(len(val_sequence)):\n",
    "    batch_x, batch_y = val_sequence[idx]  # 获取批次数据\n",
    "    \n",
    "    # 检查特征数据和标签数据中是否有 NaN 或 Inf 值\n",
    "    if check_nan_or_inf_in_batch(batch_x):\n",
    "        print(f\"NaN or Inf detected in features of batch {idx}\")\n",
    "    if check_nan_or_inf_in_batch(batch_y):\n",
    "        print(f\"NaN or Inf detected in labels of batch {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0b135-9bcd-40df-924d-7163186f0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# x,y=val_sequence[1]\n",
    "# # x 是一个列表，y 是一个标签列表的列表\n",
    "# # 假设x中的顺序与bands_count中的键顺序一致\n",
    "\n",
    "# # 计算绘图的总行数：所有输入的波段数 + 所有输出的数量\n",
    "# input_band_count = sum(bands_count.values())\n",
    "# output_label_count = sum(len(labels) for labels in y)\n",
    "# nrows = input_band_count + output_label_count\n",
    "# ncols = 1  # 因为我们只查看第一个批次的数据\n",
    "\n",
    "# fig, axes = plt.subplots(nrows, ncols, figsize=(5, nrows * 2), squeeze=False)\n",
    "\n",
    "# current_row = 0\n",
    "# # 可视化每个输入图像的所有波段\n",
    "# for input_index, (input_key, band_count) in enumerate(bands_count.items()):\n",
    "#     for band_index in range(band_count):\n",
    "#         ax = axes[current_row, 0]\n",
    "#         ax.imshow(x[input_index][0, :, :, band_index], cmap='gray')\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title(f'Input: {input_key}, Band: {band_index + 1}')\n",
    "#         current_row += 1\n",
    "\n",
    "# # 可视化每个输出标签\n",
    "# for label_set_index, label_set in enumerate(y):  # y 是标签列表的列表\n",
    "#     for label_index, label_data in enumerate(label_set):\n",
    "#         ax = axes[current_row, 0]\n",
    "#         ax.imshow(label_data[:, :, 0], cmap='viridis')  # 这里做了修正\n",
    "#         ax.axis('off')\n",
    "#         ax.set_title(f'Output Label Set: {label_set_index + 1}, Label: {label_index + 1}')\n",
    "#         current_row += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31fe476-c3fa-4398-82fd-838e2ae0b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_sequence = ValSequence(VAL_IMAGES, BATCH_SIZE, patch_sizes, bands_count, label_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ead7d-82df-43cf-8558-fc64f4e8d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置输入尺寸\n",
    "input_shape_s1 = 224,224,6  # Sentinel-1 输入尺寸\n",
    "input_shape_s10 = 224,224,4  # Sentinel-2 输入尺寸\n",
    "input_shape_s20 = 112,112,6  # Sentinel-2 输入尺寸\n",
    "input_shape_gedi = 28, 28, 4  # GEDI 输入尺寸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6db440-af6d-459b-b694-97dfb21cea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape_s1, input_shape_s10, input_shape_s20, input_shape_gedi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e45cb51-0879-4461-a924-8640a28d0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7539d-632b-4940-847a-7d2719236548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_mse(y_true, y_pred):\n",
    "    eps = K.epsilon()  # 获取一个小常数\n",
    "    return K.mean(K.square(y_pred - y_true) + eps, axis=-1)  # 将小常数加到损失计算中\n",
    "\n",
    "# 定义损失函数字典，使用自定义损失函数\n",
    "losses1 = {\n",
    "    'output_250': custom_mse,  # 使用自定义的损失函数\n",
    "    'output_100': custom_mse   # 同上\n",
    "}\n",
    "\n",
    "# 损失权重保持不变\n",
    "loss_weights1 = {\n",
    "    'output_250': 0.4,\n",
    "    'output_100': 0.6\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ce7cd-e7b1-46e9-be43-26b9b2eca278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses1 = {\n",
    "#     'output_250': 'mean_squared_error',  # 假设这是正确的层名称\n",
    "#     'output_100': 'mean_squared_error'   # 确保这里没有多余的空格\n",
    "# }\n",
    "# loss_weights1 = {\n",
    "#     'output_250': 0.7,\n",
    "#     'output_100': 0.3\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7e7d3-259c-436e-94c9-1e39479b9363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600c8bc-5b91-455b-aaf9-57efd4206c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义 R² 指标函数\n",
    "def r_squared(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d83790-bcc6-4fee-9d6b-129c56e02ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = keras.optimizers.Adam(1e-5, clipnorm=1, epsilon=1e-4)\n",
    "model.compile(optimizer=optimizer1, loss=losses1, loss_weights=loss_weights1, metrics=['mse',r_squared,'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582dd10a-42cd-4b73-be7f-d69c39303028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class DetectNaNCallback(Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        weights = self.model.get_weights()\n",
    "        if any(np.isnan(w).any() for w in weights):\n",
    "            print(f'NaN detected in weights at batch {batch}')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        weights = self.model.get_weights()\n",
    "        if any(np.isnan(w).any() for w in weights):\n",
    "            print(f'NaN detected in weights at epoch {epoch}')\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f70d2-3445-4791-8f47-93c6f40ac581",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_detect_callback = DetectNaNCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882064df-5071-459a-b732-556f03944aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 定义早停规则\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 监控的指标，常见的有'val_loss'和'val_accuracy'\n",
    "    min_delta=0.001,  # 表示被监控指标的最小变化量，只有大于这个值时才认为模型有改善\n",
    "    patience=10,  # 指定在监控指标没有改善的情况下等待的epochs数量\n",
    "    verbose=1,  # 控制输出，1表示输出早停信息\n",
    "    mode='min',  # 在监控指标为'val_loss'时，我们希望其最小化，因此设置为'min'。如果监控指标为'val_accuracy'，则设置为'max'\n",
    "    restore_best_weights=True  # 当早停发生时，是否恢复到最好的模型权重\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf465795-5d86-4a4c-b962-492807fbfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "history= model.fit(\n",
    "    train_sequence, \n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    verbose=1, \n",
    "    validation_data=val_sequence,\n",
    "    callbacks=[nan_detect_callback,early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975439d-e6c8-4e3c-8a7d-835b09527865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model training history\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "# 绘制损失曲线\n",
    "axs[0].set_title(\"Sample net training curve loss\")\n",
    "axs[0].plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "axs[0].plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "axs[0].legend()\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "\n",
    "# 绘制 MSE 曲线\n",
    "axs[1].set_title(\"Sample net training curve MSE\")\n",
    "axs[1].plot(history.history[\"mse\"], label=\"Train MSE\")\n",
    "axs[1].plot(history.history[\"val_mse\"], label=\"Validation MSE\")\n",
    "axs[1].legend()\n",
    "axs[1].set_ylabel(\"MSE\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65be8bf-2d09-4e02-b250-ab39e14770d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sequence.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865df81-af9c-4711-9e21-ee6852bdacc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = ValSequence(TEST_IMAGES, BATCH_SIZE, patch_sizes, bands_count, ['LABEL_250_DIR', 'LABEL_100_DIR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13887a3-327e-484c-8314-91fdd96ac7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(test_sequence, verbose=0)\n",
    "print(f\"\\nTest loss: {loss:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5203ab-c364-488b-82c6-b0d49fab45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the weight of the trained network\n",
    "model.save_weights(\"unet_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece171f-67a4-4412-a2ab-534935e2dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_biomass_estimation(image, regression_model, patch_size, step=！！！):\n",
    "    original_height, original_width, _ = image.shape\n",
    "    padded, (pad_height, pad_width) = pad_image_to_patch_size(image, patch_size)\n",
    "    height, width, _ = padded.shape\n",
    "    \n",
    "    predictions = np.zeros((height, width))  # 单通道预测值\n",
    "    \n",
    "    row = 0\n",
    "    while row + patch_size <= height:\n",
    "        row_patches = []\n",
    "\n",
    "        col = 0 \n",
    "        while col + patch_size <= width:\n",
    "            patch = padded[row:row + patch_size, col:col + patch_size, :]\n",
    "            row_patches.append(patch)\n",
    "            col += step\n",
    "\n",
    "        batch = np.array(row_patches)\n",
    "        row_predictions = regression_model.predict(batch, verbose=0)\n",
    "\n",
    "        col, patch_idx = 0, 0\n",
    "        while col + patch_size <= width:\n",
    "            # 对于回归，我们可能直接取预测值而不是累加\n",
    "            predictions[row:row + patch_size, col:col + patch_size] = \\\n",
    "                row_predictions[patch_idx].reshape(patch_size, patch_size)\n",
    "            col += step\n",
    "            patch_idx += 1\n",
    "\n",
    "        row += step\n",
    "\n",
    "    predictions = predictions[\n",
    "        pad_height:pad_height + original_height,\n",
    "        pad_width:pad_width + original_width\n",
    "    ]\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9810690-f947-4d87-9904-df66928f497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_IDX = 0\n",
    "image_path = os.path.join(IMAGE_DIR, VAL_IMAGES[SCENE_IDX])\n",
    "label_path = os.path.join(LABEL_DIR, VAL_IMAGES[SCENE_IDX])\n",
    "\n",
    "image = read_image(image_path, normalise=True)\n",
    "label = map_classes(read_image(label_path))\n",
    "\n",
    "prediction = apply_segmentation_with_fcn(image, model)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16, 7))\n",
    "\n",
    "axs[0].imshow(image[:, :, [2, 1, 0]])\n",
    "axs[0].set_title(\"Satellite image\")\n",
    "axs[1].imshow(label[:, :, 0])\n",
    "axs[1].set_title(\"Groundtruth\")\n",
    "axs[2].imshow(prediction)\n",
    "axs[2].set_title(\"U-Net prediction\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c23b1-31e6-43b7-9a8f-b4cf7b7ede67",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_IDX = 1\n",
    "image_path = os.path.join(IMAGE_DIR, VAL_IMAGES[SCENE_IDX])\n",
    "label_path = os.path.join(LABEL_DIR, VAL_IMAGES[SCENE_IDX])\n",
    "\n",
    "image = read_image(image_path, normalise=True)\n",
    "label = map_classes(read_image(label_path))\n",
    "\n",
    "prediction = apply_segmentation_with_fcn(image, model)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(16, 7))\n",
    "\n",
    "axs[0].imshow(image[:, :, [2, 1, 0]])\n",
    "axs[0].set_title(\"Satellite image\")\n",
    "axs[1].imshow(label[:, :, 0])\n",
    "axs[1].set_title(\"Groundtruth\")\n",
    "axs[2].imshow(prediction)\n",
    "axs[2].set_title(\"U-Net prediction\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a9310-48fe-4955-a5c0-1a0ede1b06d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3269cdb-f739-424c-a981-0fd5c4ac7344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
